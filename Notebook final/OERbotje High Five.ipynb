{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OER-Bot Themaopdracht\n",
    "\n",
    "### Notebook door Groep High Five\n",
    "### Note: Sommige code is geschreven met ChatGPT, dit zal vermeld staan in de tekst boven/ rondom de code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductie\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In dit document wordt de toepassing van GUFF met 'geitje' in combinatie met een semDB verkend. Het doel is om een chatbot te creëren die vragen kan beantwoorden over het Onderwijs- en Examenreglement (OER). We leggen stapsgewijs uit hoe we het OER-bot project hebben aangepakt volgens de CRISP-DM-methode.\n",
    "\n",
    "We beginnen met de **Business Understanding**, waarin we de casus en de eindvisie toelichten. Vervolgens bespreken we de **Data Understanding**, waar we ingaan op het aangeleverde Word-bestand en de bevindingen die we hebben gedaan. Bij de **Data Preparation** leggen we uit hoe we het Word-bestand hebben omgezet naar een bruikbaar formaat voor ons model. In de sectie **Modelling** beschrijven we hoe we zijn overgegaan van Mischa's notebook naar een Flask-app. Bij de **Evaluation** reflecteren we op het eindresultaat, en in de sectie **Deployment** bespreken we de beste vervolgstappen voor de Flask-app.\n",
    "\n",
    "Deze opdracht is afkomstig van Hogeschool Windesheim, een Nederlandse instelling voor hoger onderwijs. De hogeschool biedt een breed scala aan bachelor- en masteropleidingen, waaronder programma's in techniek en sociale wetenschappen. Hogeschool Windesheim heeft verschillende vestigingen en telt ongeveer 1800 medewerkers in totaal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Hogeschool Windesheim heeft te maken met een probleem dat studenten onvoldoende het Onderwijs- en Examenreglement (OER) bestuderen. Het OER is een belangrijk document waarin de rechten en plichten van studenten staan beschreven, evenals de regels rondom tentamens en beoordelingen. Door de complexiteit van het OER vinden studenten het echter vaak lastig om deze informatie te begrijpen en toe te passen. Dit leidt tot vragen, misverstanden en mogelijk onnodige fouten tijdens hun studie.\n",
    "\n",
    "Windesheim ondervindt dat studenten het Onderwijs- en Examenreglement (OER) onvoldoende bestuderen. Dit gebrek aan begrip kan leiden tot verwarring en onzekerheid bij studenten over hun rechten en plichten binnen het onderwijs. Om deze uitdaging aan te pakken, wil Windesheim een chatbot ontwikkelen die studenten helpt bij het beantwoorden van hun vragen over het OER. Deze chatbot biedt een gebruiksvriendelijke manier voor studenten om informatie te verkrijgen, waardoor zij beter in staat zijn om belangrijke details uit het OER tot zich te nemen.\n",
    "\n",
    "### Oplossing\n",
    "De chatbot biedt een oplossing om de toegankelijkheid van de informatie in het OER te verbeteren. Door middel van een chatbot kunnen studenten eenvoudig vragen stellen en direct antwoorden ontvangen. Dit helpt hen om op een gemakkelijkere en efficiëntere manier inzicht te krijgen in de inhoud van het OER, wat hen in staat stelt om beter geïnformeerde keuzes te maken over hun studie. De chatbot fungeert als een toegankelijke bron van informatie die altijd beschikbaar is.\n",
    "\n",
    "### Wat gaan we oplossen?\n",
    "\n",
    "We lossen op dat studenten op een eenvoudigere manier toegang hebben tot de informatie van het OER. Door een chatbot te ontwikkelen, kunnen studenten snel en effectief antwoorden krijgen op hun vragen, wat hen helpt de belangrijke documentatie beter te begrijpen en te gebruiken.\n",
    "\n",
    "### Kritische Succesfactoren\n",
    "Het project is succesvol wanneer er een volledig functionele chatbot is ontwikkeld binnen een Python-applicatie, inclusief een grafische gebruikersinterface (GUI) met Flask. Deze chatbot moet in staat zijn om relevante vragen te beantwoorden met informatie uit het OER-bestand. Daarnaast zijn de volgende kritische succesfactoren (KSF’s) van belang:\n",
    "\n",
    "1. **Nauwkeurigheid van de antwoorden:** De chatbot moet vragen correct beantwoorden door zowel de juiste informatie uit de database te halen zonder onnodige details als de context te begrijpen.\n",
    "2. **Efficiëntie van het systeem:** De chatbot moet snel en betrouwbaar antwoorden genereren, zelfs bij complexe vragen over het OER.\n",
    "3. **Gebruiksvriendelijkheid:** De applicatie moet eenvoudig te gebruiken zijn en toegankelijk voor studenten zonder technische kennis.\n",
    "4. **Onderbouwing van de keuzes:** Tijdens het ontwikkelingsproces moet de rationale achter de technische en ontwerpkeuzes duidelijk worden uitgelegd en gedocumenteerd.\n",
    "\n",
    "### Kritieke Prestatie-indicatoren (KPI’s)\n",
    "Om het succes van het project te meten, kunnen de volgende KPI's worden gebruikt:\n",
    "\n",
    "1. **Nauwkeurigheid van de antwoorden:** Zelf controleren of de antwoorden van de chatbot overeenkomen met het OER.\n",
    "2. **Efficiëntie van het systeem:** De gemiddelde tijd die de chatbot nodig heeft om een antwoord te genereren mag niet langer dan 30 seconden zijn.\n",
    "\n",
    "### Context van de vraag\n",
    "De context van de vraag is het ontwikkelen van een chatbot die studenten helpt informatie te vinden over het OER. Studenten hebben vaak moeite om deze documenten goed te begrijpen en de chatbot biedt een interactieve manier om antwoorden te verkrijgen en onduidelijkheden op te helderen. Hierdoor kunnen studenten beter geïnformeerde beslissingen nemen over hun studie.\n",
    "\n",
    "### Ontwerp en Eindvisie\n",
    "De eindvisie is om een functionele chatbot te creëren die studenten de mogelijkheid biedt om vragen te stellen over het OER. Dit zal de drempel voor het begrijpen van belangrijke informatie verlagen, waardoor studenten beter voorbereid zijn op hun studie. De chatbot moet geïntegreerd worden in een intuïtieve interface die ook op lange termijn door Windesheim eenvoudig te onderhouden en bij te werken is.\n",
    "\n",
    "### Dataset en haalbaarheid\n",
    "De dataset die is aangeleverd is het OER (Word-bestand). Er moet onderzoek worden gedaan naar hoe deze informatie effectief kan worden omgezet naar een gestructureerd formaat, zoals een database of JSON-bestand, dat de chatbot kan raadplegen. Indien dit lukt, kan de vraag worden opgelost met de gegeven dataset. Daarbij is het belangrijk om aandacht te besteden aan de juiste verwerking van de tekstuele informatie uit het OER, zodat de chatbot niet alleen tekst kan opzoeken, maar ook de betekenis en context van de vragen goed begrijpt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Voor deze themaopdracht pakken we het proces van de data understanding anders aan. We hebben een Word-bestand gekregen van het Onderwijs- en Examenreglement (OER), dat we op een zinvolle manier willen omzetten naar een semantische database (SemDB). Als eerste hebben we dit document gezamenlijk doorgenomen en gekeken naar de structuur en de inhoud.\n",
    "\n",
    "Al snel kwamen we tot de conclusie dat bepaalde onderdelen in het document lastig waren om mee te werken voor in een SemDB. Een aantal van deze onderdelen zijn:\n",
    "\n",
    "- Afbeeldingen\n",
    "- Inhoud\n",
    "- Tabellen\n",
    "- Voorblad\n",
    "\n",
    "``Afbeeldingen``: Afbeeldingen zijn voor ons model niet van toegevoegde waarde, aangezien de bot enkel tekstuele vragen over het OER gaat beantwoorden.\n",
    "\n",
    "![afbeelding](Afbeeldingen\\afbeelding.JPG)\n",
    "\n",
    "``Inhoud``: De inhoudsopgave bevat alleen kopjes en hoofdstuktitels, wat geen relevante informatie oplevert voor het model. Het is daarom belangrijk om deze in de data preparation te verwijderen.\n",
    "\n",
    "![inhoud](Afbeeldingen\\inhoud.JPG)\n",
    "\n",
    "``Tabellen``: De tabellen bevatten waardevolle informatie voor de chatbot. Echter staan de gegevens momenteel in tabelvorm, en deze moeten worden omgezet naar tekst. We moeten onderzoeken hoe we dit efficiënt kunnen omzetten.\n",
    "\n",
    "![Tabellen](Afbeeldingen\\Tabel.JPG)\n",
    "\n",
    "``Voorblad``: Het voorblad levert geen bruikbare informatie op en kan in de data preparation worden verwijderd.\n",
    "\n",
    "Verder moeten we onderzoeken of we het Word-bestand kunnen gebruiken zoals het is, of dat we het moeten converteren naar een tekstbestand (.txt).\n",
    "\n",
    "Actie punten voor data prep:\n",
    "- Afbeeldingen verwijderen.\n",
    "- Inhoud verwijderen\n",
    "- Voorblad verwijderen\n",
    "\n",
    "Daarnaast moeten we een snelle en effectieve manier vinden om de tabellen om te zetten naar tekst, aangezien deze informatie te belangrijk is om weg te laten. We moeten dus onderzoeken of we dit handmatig of automatisch kunnen uitvoeren.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Het klaarmaken van de data voor het modelleren bestond uit twee gedeeltes:\n",
    "1. Data formatteren zodat deze schoon en geordend verwerkt kan worden\n",
    "2. Aanmaken van een database van waaruit de semantische zoekopdrachten uit kunnen worden gevoerd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatteren van de data\n",
    "\n",
    "### TXT\n",
    "In eerste instantie kozen we ervoor om het Word-bestand om te zetten naar een .txt-bestand. Dit deden we omdat we de code van Mischa gebruikte die .txt-bestanden gebruikte, waardoor we snel konden testen of de chatbot werkte. Later bleek echter dat dit niet de ideale manier was. Hieronder staat de eerste data preparation, met de latere oplossing waarbij we geen .txt meer hebben gebruikt.\n",
    "\n",
    "Tijdens de data understanding ontdekten we dat bepaalde onderdelen uit het document verwijderd moesten worden. Aangezien het weinig tijd kostte om afbeeldingen, het voorblad en de inhoudsopgave te verwijderen, besloten we dit handmatig te doen.\n",
    "\n",
    "Nadat we deze overbodige stukken tekst hadden verwijderd, hebben we het Word-bestand via Word omgezet naar een .txt-bestand. Het omzetten naar .txt verliep echter niet soepel, en het bestand was slecht leesbaar, zoals te zien is op de afbeelding.\n",
    "\n",
    "![poging1](Afbeeldingen\\1erun.JPG)\n",
    "\n",
    "In het .txt-bestand zagen we al snel dat de tabellen niet zichtbaar waren, dus hebben we de tabellen omgezet naar tekst met een functie binnen Word. Ook dit werkte niet helemaal goed, dus hebben we ChatGPT gevraagd het .txt-bestand netjes om te zetten naar een .txt-bestand. Dit zag er al een stuk beter uit, maar het was nog steeds niet wat we wilden bereiken.\n",
    "\n",
    "![poging2](Afbeeldingen\\optie2.JPG)\n",
    "\n",
    "Na deze stap hebben we gekeken hoe Mischa's code werkt met een .txt-bestand, en op basis daarvan hebben we het .txt-bestand handmatig aangepast en getest met zijn spellingsmodel. Het .txt-bestand zag er uiteindelijk als volgt uit.\n",
    "\n",
    "![poging3](Afbeeldingen\\3erun.JPG)\n",
    "\n",
    "We merkten echter dat het model weinig uit het .txt-bestand haalde en voornamelijk op voorspellingen werkte. Dit kwam waarschijnlijk doordat de tekst niet goed per kopje op een lijn stond. Het model maakte geen gebruik van de informatie uit het .txt-bestand, dus zijn we verder gaan onderzoeken naar betere opties om tot een beter resultaat te komen.\n",
    "\n",
    "### JSON\n",
    "\n",
    "Uiteindelijk hebben we de oplossing gevonden via de volgende [generator website](https://cloud.llamaindex.ai/parse). We hebben het gefilterde .docx-bestand zonder afbeeldingen en voorblad geüpload en omgezet naar een JSON-bestand. We kozen uiteindelijk voor JSON boven .txt of .md, omdat we een [script](https://github.com/dodeeric/json-files-ingestion-into-chroma-vector-db/blob/main/app.py) vonden dat JSON-bestanden kon verwerken.\n",
    "\n",
    "Na verder onderzoek bleek ook dat JSON gemakkelijker de hoofdstukken op een logische manier indeelt, iets wat niet goed gebeurde in het .txt-bestand.\n",
    "\n",
    "Aangezien SemDB voor ons allemaal nieuw was, hebben we het proces misschien niet optimaal aangepakt, maar dit was voor ons nodig om uiteindelijk een bruikbare manier te vinden om het .docx-document om te zetten naar een JSON-bestand dat geschikt is voor onze chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aanmaken van een Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De volgende code is geïnspireerd door [Mischa's artikel op Medium](https://medium.com/@xvtjy/rag-implementation-using-keras-nlp-and-chromadb-34c6868dd908), we hebben eerst zijn code werkend gekregen met de spells. Daarna hebben we ons eerste concept dus draaiende gekregen met het `.txt-bestand`.\n",
    "\n",
    "Omdat niet alle code even logisch was hebben we met **ChatGPT** overal comments aan toegevoegd om beter de code te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de Llama library voor het werken met Llama-modellen van Llama.cpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Import chromadb voor het werken met een database voor het opslaan en ophalen van vectoren en documenten\n",
    "import chromadb\n",
    "\n",
    "# Waarschuwingen negeren om een schonere uitvoer te krijgen\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pipeline van Transformers voor het eenvoudig implementeren van verschillende NLP-taken\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lijsten om documenten, metadata en ID's op te slaan\n",
    "documents = []\n",
    "metadata = []\n",
    "ids = []\n",
    "counter = 0\n",
    "\n",
    "# Open het tekstbestand met een specifieke encoding\n",
    "with open('data/Opleidngsdeel_OER_HBO-ICT_Zwolle_2024-2025_Filterd_Modified_Long_Lines.txt', 'r', encoding='utf-8') as lines:\n",
    "    # Loop door elke regel in het bestand\n",
    "    for line in lines:\n",
    "        # Splits de regel op basis van het scheidingsteken \" - \"\n",
    "        line = line.split(\" - \")  \n",
    "        \n",
    "        # Voeg het tweede deel van de gesplitste lijn toe aan de metadata als een dictionary\n",
    "        metadata.append({'kopje': line[1]})  \n",
    "        \n",
    "        # Voeg het eerste deel van de gesplitste lijn toe aan de documentenlijst\n",
    "        documents.append(line[0])  \n",
    "        \n",
    "        # Voeg een unieke ID toe aan de IDs-lijst op basis van de teller\n",
    "        ids.append(str(counter))  \n",
    "        \n",
    "        # Verhoog de teller voor de volgende ID\n",
    "        counter += 1  \n",
    "\n",
    "# Print de lijst van ID's naar de console\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als database hebben wij gekozen voor ChromaDB. Dit is conform de voorbeelduitwerking en het werkt naar behoren. Wij hebben wel geprobeerd andere semantische databases op te zoeken of te benaderen (er is contact gezocht met semanticdb.ai), maar vanwege de scope van de opdracht hebben wij hier geen wijzigingen aan toegevoegd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een client aan voor de Chroma database\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Verkrijg of maak een collectie met de naam \"information\"\n",
    "collection = client.get_or_create_collection(\"information\")\n",
    "\n",
    "# Voeg de documenten, metadata en ID's toe aan de collectie\n",
    "collection.add(\n",
    "    documents=documents,    # De lijst van documenten die zijn ingeladen\n",
    "    metadatas=metadata,     # De bijbehorende metadata voor de documenten\n",
    "    ids=ids                 # De unieke ID's voor elk document\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Wij hebben het volgende model gekozen:\n",
    "- GEITje-7B-ultra-GGUF\n",
    "\n",
    "Op aanraden van de docenten zijn wij begonnen met hoe wij GEITje kunnen gebruiken. Dit is een Nederlandse versie van mistral LLM. Voor ons was het logisch om alleen GEITje te onderzoeken omdat wij een Nederlandse chatbot willen maken en daarvoor dus een Nederlandse LLM nodig hebben.\n",
    "\n",
    "Het viel ons op dat er meerdere versies van GEITje aanwezig zijn. Wij hebben daarom ook gekeken naar het installatieproces. Voor veel modellen van HuggingFace hebben wij de package \"transformers\" nodig. Niet iedereen kreeg die aan de praat, en daarom hebben wij de package \"llama_cpp-python\" gebruikt. Deze is compitabel met de GEITje-7B-ultra-GGUF llm van BramVanroy.\n",
    "\n",
    "Onderstaande code is dan van HuggingFaces afgehaald om de llm op te halen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad een voorgetraind Llama-model\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"BramVanroy/GEITje-7B-ultra-GGUF\",  # Het ID van de modelrepository\n",
    "    filename=\"geitje-7b-ultra-q8_0.gguf\",         # Het bestand van het model\n",
    ")\n",
    "\n",
    "# Maak een chat-completion aan met een voorbeeldbericht van de gebruiker\n",
    "llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",                    # Rol van de afzender (hier: gebruiker)\n",
    "            \"content\": \"Wat is de hoofdstad van Frankrijk?\"  # Voorbeeldvraag\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder vragen wij de gebruiker om input en wordt een voorgestelde antwoord uit de ChromaDB opgehaald"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vraag de gebruiker om een vraag in te voeren\n",
    "user_input = input(\"Wat is jouw vraag?: \")\n",
    "\n",
    "# Voer een query uit op de collectie met de gebruikersvraag\n",
    "results = collection.query(\n",
    "    query_texts=[user_input],  # De tekst van de gebruikersvraag\n",
    "    n_results=1                 # Het aantal resultaten dat moet worden opgehaald\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwerk de eerste metadata en het eerste document uit de resultaten\n",
    "result = results['metadatas'][0][0]['kopje'] + \" - \" + results['documents'][0][0][:-1]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In het onderstaande wordt het voorgestelde antwoord meegegeven in een prompt aan het GEITje LLM. Dit is conform het RAG principe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een prompt voor het Llama-model op basis van de resultaten en gebruikersinvoer\n",
    "prompt = f\"\"\"\n",
    "voorgestelde antwoord: {result}\n",
    "User input: {user_input}\n",
    "stel een nieuw antwoord voor op basis van het voorgestelde antwoord.\n",
    "\"\"\"\n",
    "\n",
    "# Print de prompt om te zien wat naar het model wordt gestuurd\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "# Genereer een nieuw antwoord met het Llama-model op basis van de gemaakte prompt\n",
    "inference = llm(prompt, max_tokens=100)  # Beperk het aantal tokens in de output\n",
    "generated_text = inference['choices'][0]['text']  # Verkrijg de gegenereerde tekst\n",
    "print(\"Inference: \", generated_text)  # Print de gegenereerde output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask Applicatie\n",
    "\n",
    "Nadat deze code van Mischa werkte met het .txt bestand zijn we begonnen met het kijken om de Data Preparation te verbeteren. Dat is toen we erachter kwamen dat we hiervoor [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) konden gebruiken. Met deze website hebben we dus het `.json` bestand gegenereerd die we hebben gebruikt voor de Flask Applicatie.\n",
    "\n",
    "### Flask App Structuur voor de OERbotje Chatbot\n",
    "\n",
    "We hebben een bepaalde structuur gemaakt voor de app omdat we styling (.html/ .css) hebben toegepast. De structuur ziet er als volgt uit:\n",
    "\n",
    "```markdown\n",
    "flask_app/\n",
    "│\n",
    "├── app.py                         # Hoofd Python-bestand met de Flask-applicatie\n",
    "├── requirements.txt               # Lijst met benodigde Python-pakketten\n",
    "│\n",
    "├── templates/                     # Bevat HTML-sjablonen\n",
    "│   └── index.html                 # Hoofd HTML-bestand voor de app (Bevat HTML, CSS en JavaScript)\n",
    "│\n",
    "└── data/                          # Bevat de gegevensbestanden\n",
    "    └── OER.json                   # JSON-bestand met OER-gegevens\n",
    "```\n",
    "\n",
    "Het framework van de app hebben we gegenereerd met **ChatGPT**. We hebben deze keuze gemaakt omdat we nog nooit met 'Flask' hebben gewerkt. Na wat onderzoek ernaar hebben we dit meegegooid in een prompt samen met de casus en het notebook hierboven. met de instructie om een simpele Flask-app hiervan te maken. Dit resulteerde in een simpele Flask-app bouwen met een eenvoudige HTML-pagina waarin getypt kon worden en een knop aanwezig was om op te drukken om een antwoord te genereren.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"afbeeldingen/eerste_flaskapp.png\" alt=\"Eerste Flask App\" width=\"600\"/>\n",
    "    <figcaption>Hoe de eerste versie van de flask app eruit zag.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "Een groot punt nadat we de Flask-App draaiende hadden was om het antwoord te verbeteren met `Prompt Engineering;` het model hallucineerde best vaak wanneer hij niet wist waar hij in het OER moest zoeken. We hebben hem uiteindelijk daarom deze prompt meegegeven:\n",
    "\n",
    "```markdown\n",
    "*\"Je bent een chatbot die studenten helpt met vragen over het Onderwijs- en Examenreglement (OER) van Windesheim, specifiek voor de opleiding HBO-ICT.Beschrijf dit in maximaal 3 duidelijke zinnen. Zorg ervoor dat de output geen halve zinnen of ongewenste tekens aan het begin van de tekst bevat. Geef het antwoord zonder extra inleiding of kopjes en zorg ervoor dat het een volledige en samenhangende tekst is. Focus je alleen op zelfstandignaamwoorden in de vraag. Je mag niet halliciuneren. Als er geen antwoord op de vraag is geef je 'Geen resultaten gevonden'.*\"\n",
    "```\n",
    "\n",
    "Dit was iets waar we wel veel mee hebben geknutseld. Uiteindelijk waren we wel vrij tevreden met het resultaat. Uiteindelijk hebben we de interface van de app verbeterd. Hiervoor hebben we wel **ChatGPT** gebruikt omdat Flask nieuw voor ons was en omdat we allemaal weinig ervaring hebben met HTML, CSS en JavaScript.\n",
    "\n",
    "De belangrijkste dingen die we hebben toegevoegd aan de interface is dat je kan zien dat het model bezig is met een antwoord genereren en dat je de chat geschiedenis kan zien. Niet te verwarren met het feit dat het model de chat geschiedenis onthoudt bij het beantwoorden van nieuwe vragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "## Screenshots resultaten toevoegen\n",
    "## Terugkoppelen KPI's\n",
    "\n",
    "Bij het evalueren van de prestaties van onze Flask-app en de onderliggende chatbot zijn er enkele belangrijke aandachtspunten naar voren gekomen.\n",
    "\n",
    "### Onbetrouwbare Informatiebronnen\n",
    "Eén van de grootste uitdagingen is dat het model soms niet de OER als bron gebruikt en in plaats daarvan zelf antwoorden verzint. Dit leidt tot antwoorden die simpelweg niet kloppen,  vooral omdat de chatbot bedoeld is om studenten nauwkeurige en relevante informatie over het Onderwijs- en Examenreglement (OER) te bieden. Ondanks onze inspanningen om `Prompt Engineering` toe te passen, blijft het model af en toe antwoorden genereren die niet op informatie uit het OER zijn gebaseerd.\n",
    "\n",
    "### Hallucinatie en Onvolledige Antwoorden\n",
    "Daarnaast hebben we gemerkt dat het model nog steeds soms hallucinaties vertoont. Soms geeft het model geen volledige zinnen terug, wat resulteert in afgekapte of onduidelijke antwoorden. \n",
    "\n",
    "### Tijdsduur voor Antwoordgeneratie\n",
    "Een ander belangrijk punt is de tijd die het model nodig heeft om antwoorden te genereren. Op dit moment duurt het ongeveer `een minuut per vraag` voordat een antwoord wordt teruggegeven. De lange wachttijd kan worden veroorzaakt door verschillende factoren, waaronder de complexiteit van de vragen, de omvang van de dataset waarmee het model werkt en de prestaties van de laptop waarop de Flask-app is gehost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Met de ontwikkeling van onze Flask-app zijn we ons ervan bewust dat er nog veel kansen zijn voor verdere verbetering en uitbreiding van het model:\n",
    "\n",
    "### Chatgeschiedenis\n",
    "Een eerste stap die we willen nemen, is het implementeren van een functie voor `chatgeschiedenis`. Dit zou het model in staat stellen om eerder gestelde vragen te onthouden en deze contextueel mee te nemen in de antwoorden. Hierdoor kan de chatbot relevanter en nauwkeuriger reageren op opvolgende vragen.\n",
    "\n",
    "### Meerdere Documenten Ondersteunen\n",
    "Daarnaast willen we het model uitbreiden zodat het niet alleen een OER-bot is voor de opleiding HBO-ICT, maar ook voor `andere studies`. Dit zou je kunnen doen door een structuur te ontwikkelen die het mogelijk maakt om meerdere documenten eenvoudig te integreren. Dit vereist een uitgebreidere app dan wat we momenteel hebben, maar het zou een waardevolle uitbreiding zijn die ons bereik en de functionaliteit van de chatbot aanzienlijk vergroot.\n",
    "\n",
    "### Hosting op een Externe Server\n",
    "Momenteel draait onze app nog op `localhost`, maar we overwegen om deze te hosten op een server van Windesheim, zoals Skylab of een vergelijkbare omgeving. Dit zou de toegankelijkheid van de app verbeteren en het mogelijk maken dat meer studenten gebruik kunnen maken van de chatbot.\n",
    "\n",
    "### Verbetering van het Model\n",
    "Ten slotte moet ons model in het algemeen verder `worden verbeterd`. Het is cruciaal dat de chatbot alleen antwoorden genereert op basis van de beschikbare documentatie, in dit geval de OER. Als het model geen relevant antwoord kan vinden, moet het eenvoudigweg aangeven dat het de vraag niet begrijpt en aanmoedigen om deze anders of eenvoudiger te formuleren. Deze aanpak zou niet alleen hallucinaties verminderen, maar ook de betrouwbaarheid van de chatbot verhogen, waardoor het een betrouwbaarder hulpmiddel wordt voor studenten.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
